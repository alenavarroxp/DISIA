{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82500582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 1. PRELIMINARES\n",
    "\n",
    "# %%\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# %%\n",
    "# GPU configuration\n",
    "device_type = \"GPU\"\n",
    "devices = tf.config.list_physical_devices(device_type)\n",
    "if not devices:\n",
    "    raise RuntimeError(f\"No {device_type} devices are used in the host.\")\n",
    "\n",
    "# %%\n",
    "seed = 42\n",
    "\n",
    "# %% [markdown]\n",
    "# # 2. CARGA DE DATOS\n",
    "\n",
    "# %%\n",
    "# Rutas de los datos\n",
    "TRAIN_CSV = '/kaggle/input/dataset/labels/train_data.csv'\n",
    "TRAIN_IMG_DIR = '/kaggle/input/dataset/images'\n",
    "TEST_CSV = '/kaggle/input/test-all/labels/test_data.csv'\n",
    "TEST_IMG_DIR = '/kaggle/input/test-all/images'\n",
    "\n",
    "# %%\n",
    "batch_size = 32\n",
    "image_size = (224, 224)  # EfficientNetB0 input size\n",
    "\n",
    "# Load CSV data\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Create combined labels for stratified split\n",
    "train_df['combined_label'] = train_df['transitable'].astype(str) + \"_\" + train_df['Inundado'].astype(str)\n",
    "\n",
    "# Split into train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=seed,\n",
    "    stratify=train_df['combined_label']\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# # 3. PREPARACIÓN DE DATOS\n",
    "\n",
    "# %%\n",
    "def create_dataset(df, img_dir, batch_size, shuffle=True):\n",
    "    def load_and_process_image(file_path, label):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, image_size)\n",
    "        img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "        return img, {'transitable': label[0], 'inundado': label[1]}\n",
    "    \n",
    "    # Create file paths and labels\n",
    "    file_paths = [os.path.join(img_dir, f\"{id}.jpg\") for id in df['ID']]\n",
    "    labels = df[['transitable', 'Inundado']].values.astype(np.float32)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(file_paths), seed=seed)\n",
    "    \n",
    "    dataset = dataset.map(load_and_process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# For test dataset\n",
    "test_file_paths = [os.path.join(TEST_IMG_DIR, f\"{id}.jpg\") for id in test_df['ID']]\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_file_paths)\n",
    "\n",
    "def load_and_process_image_test(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "test_ds = test_ds.map(load_and_process_image_test, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "# %% [markdown]\n",
    "# # 4. CARGA DE MODELOS (VERSIÓN CORREGIDA)\n",
    "\n",
    "# %%\n",
    "model_files = [\n",
    "    \"/kaggle/input/modelo_20250329_184118/keras/default/1/modelo_20250329_184118.keras\"\n",
    "]\n",
    "\n",
    "# SOLUCIÓN 1: Usar tf.keras en lugar de keras directo\n",
    "try:\n",
    "    models = [tf.keras.models.load_model(model_file) for model_file in model_files]\n",
    "    print(\"MODELOS CARGADOS CON tf.keras.models.load_model\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar con tf.keras: {e}\")\n",
    "    \n",
    "    # SOLUCIÓN 2: Usar TFSMLayer para Keras 3\n",
    "    try:\n",
    "        from keras.layers import TFSMLayer\n",
    "        from keras import Input, Model\n",
    "        \n",
    "        models = []\n",
    "        for model_file in model_files:\n",
    "            # Crear capa que envuelve el SavedModel\n",
    "            sm_layer = TFSMLayer(model_file, call_endpoint='serving_default')\n",
    "            \n",
    "            # Reconstruir el modelo completo\n",
    "            inputs = Input(shape=(224, 224, 3))\n",
    "            outputs = sm_layer(inputs)\n",
    "            model = Model(inputs, outputs)\n",
    "            models.append(model)\n",
    "        \n",
    "        print(\"MODELOS CARGADOS COMO TFSMLayer\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error al cargar con TFSMLayer: {e2}\")\n",
    "        raise\n",
    "\n",
    "# %% [markdown]\n",
    "# # 5. PREDICCIÓN PARA MODELO MULTI-SALIDA (ACTUALIZADO)\n",
    "\n",
    "# %%\n",
    "def predict_multi_output_model(model, test_ds):\n",
    "    try:\n",
    "        # Get predictions\n",
    "        predictions = model.predict(test_ds)\n",
    "        \n",
    "        # Manejar diferentes formatos de salida\n",
    "        if isinstance(predictions, dict):\n",
    "            # Formato de diccionario (common en SavedModel)\n",
    "            transitable_probs = predictions.get('transitable', predictions.get('output_0')).flatten()\n",
    "            inundado_probs = predictions.get('inundado', predictions.get('output_1')).flatten()\n",
    "        elif isinstance(predictions, list):\n",
    "            # Formato de lista\n",
    "            transitable_probs = predictions[0].flatten()\n",
    "            inundado_probs = predictions[1].flatten()\n",
    "        else:\n",
    "            # Formato único (puede ser tensor)\n",
    "            transitable_probs = predictions[:, 0].flatten()\n",
    "            inundado_probs = predictions[:, 1].flatten()\n",
    "        \n",
    "        # Convertir a predicciones binarias\n",
    "        transitable_preds = (transitable_probs > 0.5).astype(int)\n",
    "        inundado_preds = (inundado_probs > 0.5).astype(int)\n",
    "        \n",
    "        # Crear etiquetas descriptivas\n",
    "        labels = []\n",
    "        for t, i in zip(transitable_preds, inundado_preds):\n",
    "            if t == 1 and i == 0:\n",
    "                labels.append(\"transitable_si-inundado_no\")\n",
    "            elif t == 0 and i == 1:\n",
    "                labels.append(\"transitable_no-inundado_si\")\n",
    "            elif t == 1 and i == 1:\n",
    "                labels.append(\"transitable_si-inundado_si\")\n",
    "            else:\n",
    "                labels.append(\"transitable_no-inundado_no\")\n",
    "        \n",
    "        return labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error en predict_multi_output_model: {e}\")\n",
    "        raise\n",
    "# %% [markdown]\n",
    "# # 6. ENSAMBLE Y GUARDADO DE RESULTADOS\n",
    "\n",
    "# %%\n",
    "def ensemble_predict_and_save(models, test_ds, output_path):\n",
    "    # For single model case (no actual ensemble needed)\n",
    "    if len(models) == 1:\n",
    "        model = models[0]\n",
    "        if isinstance(model.output, list) and len(model.output) > 1:\n",
    "            predictions = predict_multi_output_model(model, test_ds)\n",
    "        else:\n",
    "            probabilities = model.predict(test_ds)\n",
    "            predictions = np.argmax(probabilities, axis=1)\n",
    "            if hasattr(model, 'class_names'):\n",
    "                predictions = [model.class_names[p] for p in predictions]\n",
    "        \n",
    "        # Prepare results\n",
    "        identifiers = test_df['ID'].values\n",
    "        data = {\"Id\": identifiers, \"Label\": predictions}\n",
    "        submission = pd.DataFrame(data).set_index(\"Id\")\n",
    "        submission.to_csv(output_path)\n",
    "        return\n",
    "    \n",
    "    # For multiple models (ensemble)\n",
    "    all_predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        if isinstance(model.output, list) and len(model.output) > 1:\n",
    "            predictions = predict_multi_output_model(model, test_ds)\n",
    "        else:\n",
    "            probabilities = model.predict(test_ds)\n",
    "            predictions = np.argmax(probabilities, axis=1)\n",
    "            if hasattr(model, 'class_names'):\n",
    "                predictions = [model.class_names[p] for p in predictions]\n",
    "        \n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    # Majority voting\n",
    "    final_predictions = []\n",
    "    for preds in zip(*all_predictions):\n",
    "        most_common = Counter(preds).most_common(1)[0][0]\n",
    "        final_predictions.append(most_common)\n",
    "    \n",
    "    # Save results\n",
    "    identifiers = test_df['ID'].values\n",
    "    data = {\"Id\": identifiers, \"Label\": final_predictions}\n",
    "    submission = pd.DataFrame(data).set_index(\"Id\")\n",
    "    submission.to_csv(output_path)\n",
    "\n",
    "# %% [markdown]\n",
    "# # 7. EJECUCIÓN\n",
    "\n",
    "# %%\n",
    "# Execute prediction\n",
    "ensemble_predict_and_save(models, test_ds, \"submission.csv\")\n",
    "print(\"Predicciones completadas y guardadas en submission.csv\")\n",
    "\n",
    "# Show sample predictions\n",
    "sample = pd.read_csv(\"submission.csv\")\n",
    "print(\"\\nMuestra de predicciones:\")\n",
    "print(sample.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7108576,
     "sourceId": 64826,
     "sourceType": "competition"
    },
    {
     "datasetId": 4174999,
     "sourceId": 7214759,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4177217,
     "sourceId": 7217694,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 717.89829,
   "end_time": "2023-12-20T20:16:24.979936",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-20T20:04:27.081646",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
